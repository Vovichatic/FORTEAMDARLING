{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "все решение выполнялось в гугл колабе на видеокарте T4"
      ],
      "metadata": {
        "id": "-UA_8D3KSCoB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "1oTvimmyCW3m",
        "outputId": "c3ba1af7-9ee7-4179-e963-86ffb9d03d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.12/dist-packages (3.31.10)\n",
            "Collecting cmake\n",
            "  Downloading cmake-4.2.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.5 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmake-4.2.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (28.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.9/28.9 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip, cmake\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.31.10\n",
            "    Uninstalling cmake-3.31.10:\n",
            "      Successfully uninstalled cmake-3.31.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cmake-4.2.0 pip-25.3 setuptools-80.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "4816dce286c249f79502789b77b0172b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 1. Обновляем pip и устанавливаем инструменты сборки (это помогает при компиляции)\n",
        "!pip install --upgrade pip setuptools wheel cmake"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ПОСЛЕ ЭТОЙ ЯЧЕЙКИ НАДО ПЕРЕЗАПУСТИТЬ КЭРНЕЛ ЕСЛИ ВЫ В КОЛАБЕ"
      ],
      "metadata": {
        "id": "g6TtLqt7Rx55"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8tzYUWSCw0u",
        "outputId": "198cdbcc-6647-4d0c-87ed-fd778cb4c423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Cython in /usr/local/lib/python3.12/dist-packages (3.0.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMKNUGu2CzNf",
        "outputId": "d2f1ae2e-674b-44f0-8cd5-35ac8ec408a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting implicit\n",
            "  Downloading implicit-0.7.2.tar.gz (70 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from implicit) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.12/dist-packages (from implicit) (1.16.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from implicit) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.12/dist-packages (from implicit) (3.6.0)\n",
            "Building wheels for collected packages: implicit\n",
            "  Building wheel for implicit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for implicit: filename=implicit-0.7.2-cp312-cp312-linux_x86_64.whl size=10797912 sha256=805d4057a90c570e2910d0d063ff35e3d7b14c7bef2e0409c1d607fe3e85a91d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/00/4f/9ff8af07a0a53ac6007ea5d739da19cfe147a2df542b6899f8\n",
            "Successfully built implicit\n",
            "Installing collected packages: implicit\n",
            "Successfully installed implicit-0.7.2\n"
          ]
        }
      ],
      "source": [
        "# 2. Устанавливаем библиотеки по очереди (так надежнее)\n",
        "!pip install catboost\n",
        "!pip install pandas numpy scikit-learn\n",
        "\n",
        "# 3. Устанавливаем implicit с флагом verbose, чтобы видеть процесс (если будет компилироваться долго)\n",
        "# Обычно после установки cmake дело идет быстрее\n",
        "!pip install implicit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ПРЕДЫДУЩАЯ ЯЧЕЙКА МОЖЕТ ВЫПОЛНЯТЬСЯ ДО 10 МИНУТ (В КОЛАБЕ)"
      ],
      "metadata": {
        "id": "HE0sqadRR5io"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7IwLLguC7_A",
        "outputId": "c4c7f150-fd17-4b13-87a5-c414e494a92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Strict Config loaded.\n",
            "Config loaded.\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sparse\n",
        "import implicit\n",
        "from catboost import CatBoostRanker, Pool\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# --- MASTER CONFIGURATION  ---\n",
        "class Config:\n",
        "    RANDOM_STATE = 42\n",
        "    VAL_SIZE = 0.1\n",
        "\n",
        "    # Настройки ALS\n",
        "    ALS_FACTORS = 64\n",
        "    ALS_REGULARIZATION = 0.01\n",
        "    ALS_ITERATIONS = 20\n",
        "    N_FOLDS_ALS = 5\n",
        "\n",
        "    # Генерация кандидатов\n",
        "    TOP_K_CANDIDATES = 40     # Hard Negatives от ALS\n",
        "    NUM_RANDOM_NEGS = 0     # Random Negatives (не нужен)\n",
        "\n",
        "    # Настройки NLP\n",
        "    USE_BERT = True        # Включено\n",
        "    BERT_MODEL = 'deepvk/USER-bge-m3'\n",
        "    BERT_PCA_COMPONENTS = 32\n",
        "\n",
        "\n",
        "    USE_TFIDF=True\n",
        "    TFIDF_MAX_FEATURES=2000\n",
        "    TFIDF_SVD_COMPONENTS=32\n",
        "\n",
        "\n",
        "\n",
        "    # Фичи\n",
        "    USE_META_FEATURES = True\n",
        "    USE_LOO_STATS = True      # Оставляем Leave-One-Out (безопасно)\n",
        "    USE_TIME_FEATURES = True\n",
        "\n",
        "    CAT_COLS = ['author_id', 'publisher', 'language', 'category_id']\n",
        "\n",
        "CFG = Config()\n",
        "print(\"Strict Config loaded.\")\n",
        "\n",
        "# Функция оптимизации памяти (Обязательная)\n",
        "def reduce_mem_usage(df):\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type != object and not str(col_type).startswith('datetime'):\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "    print(f'Memory usage reduced to {df.memory_usage().sum() / 1024**2:.2f} MB')\n",
        "    return df\n",
        "\n",
        "print(\"Config loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr0fdU_8DI0s",
        "outputId": "5814ae4d-96fa-4d9f-a733-8d3919b0e62d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Попытка загрузить book_descriptions с engine='python'...\n",
            "Успешно! Загружено строк: 55784\n",
            "Memory usage reduced to 4.62 MB\n",
            "Memory usage reduced to 0.04 MB\n",
            "Memory usage reduced to 1.81 MB\n",
            "Users: 7289, Items: 55784\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading data...\")\n",
        "# Замените пути на свои\n",
        "train_df = pd.read_csv('train.csv', parse_dates=['timestamp'])\n",
        "candidates_df = pd.read_csv('candidates.csv')\n",
        "users_df = pd.read_csv('users.csv')\n",
        "books_df = pd.read_csv('books.csv')\n",
        "try:\n",
        "    print(\"Попытка загрузить book_descriptions с engine='python'...\")\n",
        "    book_descriptions = pd.read_csv(\n",
        "        \"book_descriptions.csv\",\n",
        "        engine='python',         # Более медленный, но умный движок\n",
        "        on_bad_lines='skip',     # Пропустить битые строки\n",
        "        encoding_errors='replace' # Заменить кривые символы\n",
        "    )\n",
        "    print(f\"Успешно! Загружено строк: {len(book_descriptions)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке описаний: {e}\")\n",
        "    # Создаем пустой датафрейм, чтобы код дальше не падал\n",
        "    book_descriptions = pd.DataFrame(columns=['book_id', 'description'])\n",
        "\n",
        "# Оптимизация\n",
        "train_df = reduce_mem_usage(train_df)\n",
        "users_df = reduce_mem_usage(users_df)\n",
        "books_df = reduce_mem_usage(books_df)\n",
        "\n",
        "# Глобальные маппинги ID (важно для ALS)\n",
        "unique_users = train_df['user_id'].unique()\n",
        "unique_items = books_df['book_id'].unique()\n",
        "\n",
        "user2idx = {u: i for i, u in enumerate(unique_users)}\n",
        "item2idx = {i: k for k, i in enumerate(unique_items)}\n",
        "idx2user = {i: u for u, i in user2idx.items()}\n",
        "idx2item = {k: i for i, k in item2idx.items()}\n",
        "\n",
        "num_users = len(unique_users)\n",
        "num_items = len(unique_items)\n",
        "\n",
        "print(f\"Users: {num_users}, Items: {num_items}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA-H7RjcDPuH"
      },
      "outputs": [],
      "source": [
        "def process_text_features(desc_df):\n",
        "    print(\"--- Processing Text Features ---\")\n",
        "    desc_df['description'] = desc_df['description'].fillna('')\n",
        "    final_emb_df = pd.DataFrame({'book_id': desc_df['book_id']})\n",
        "\n",
        "    # 1. TF-IDF + SVD\n",
        "    if CFG.USE_TFIDF:\n",
        "        print(\"Running TF-IDF + SVD...\")\n",
        "        tfidf = TfidfVectorizer(max_features=CFG.TFIDF_MAX_FEATURES, stop_words='english')\n",
        "        tfidf_mat = tfidf.fit_transform(desc_df['description'])\n",
        "\n",
        "        svd = TruncatedSVD(n_components=CFG.TFIDF_SVD_COMPONENTS, random_state=CFG.RANDOM_STATE)\n",
        "        svd_mat = svd.fit_transform(tfidf_mat)\n",
        "\n",
        "        cols = [f'tfidf_svd_{i}' for i in range(CFG.TFIDF_SVD_COMPONENTS)]\n",
        "        temp_df = pd.DataFrame(svd_mat, columns=cols)\n",
        "        final_emb_df = pd.concat([final_emb_df, temp_df], axis=1)\n",
        "\n",
        "    # 2. BERT + PCA\n",
        "    if CFG.USE_BERT:\n",
        "        print(f\"Running BERT ({CFG.BERT_MODEL})...\")\n",
        "        model_bert = SentenceTransformer(CFG.BERT_MODEL)\n",
        "        # Если есть GPU, используем его\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        model_bert.to(device)\n",
        "\n",
        "        # Получаем эмбеддинги (это может занять время, но rubert-tiny быстрый)\n",
        "        embeddings = model_bert.encode(\n",
        "            desc_df['description'].tolist(),\n",
        "            show_progress_bar=True,\n",
        "            batch_size=64,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        # Сжимаем PCA\n",
        "        print(\"Reducing BERT dimensions with PCA...\")\n",
        "        pca = PCA(n_components=CFG.BERT_PCA_COMPONENTS, random_state=CFG.RANDOM_STATE)\n",
        "        bert_pca = pca.fit_transform(embeddings)\n",
        "\n",
        "        cols = [f'bert_pca_{i}' for i in range(CFG.BERT_PCA_COMPONENTS)]\n",
        "        temp_df = pd.DataFrame(bert_pca, columns=cols)\n",
        "        final_emb_df = pd.concat([final_emb_df, temp_df], axis=1)\n",
        "\n",
        "    return reduce_mem_usage(final_emb_df)\n",
        "\n",
        "# Запускаем обработку текста\n",
        "if book_descriptions is not None:\n",
        "    text_features_df = process_text_features(book_descriptions)\n",
        "else:\n",
        "    text_features_df = None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "генерация эмбедингов этой моделью может длится в районе 40-50 минут (на т4)"
      ],
      "metadata": {
        "id": "S4gIfiSdSMCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def create_rich_content_features():\n",
        "    print(\"--- Creating Rich Content Features (Genres, Authors, Descs) ---\")\n",
        "\n",
        "    # 1. Загрузка данных (пути могут отличаться)\n",
        "    books = pd.read_csv('books.csv')\n",
        "    book_genres = pd.read_csv('book_genres.csv')\n",
        "    genres = pd.read_csv('genres.csv')\n",
        "\n",
        "    # === БЛОК 1: ЖАНРЫ (TF-IDF + SVD) ===\n",
        "    # Жанры - это связь \"многие-ко-многим\". Схлопнем их в строку.\n",
        "    print(\"Processing Genres...\")\n",
        "\n",
        "    # Соединяем ID с названиями\n",
        "    bg_full = book_genres.merge(genres, on='genre_id', how='left')\n",
        "\n",
        "    # Группируем жанры для каждой книги в одну строку через пробел\n",
        "    # Пример: \"Фантастика Научная_фантастика Космическая_опера\"\n",
        "    # Заменяем пробелы в названиях жанров на подчеркивания, чтобы TF-IDF не разбил их\n",
        "    bg_full['genre_name'] = bg_full['genre_name'].fillna('unknown').apply(lambda x: x.replace(' ', '_'))\n",
        "\n",
        "    genre_strings = bg_full.groupby('book_id')['genre_name'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "    genre_strings.columns = ['book_id', 'genre_str']\n",
        "\n",
        "    # TF-IDF на жанрах\n",
        "    tfidf_g = TfidfVectorizer(token_pattern=r'(?u)\\b\\w+\\b') # Простой токенайзер\n",
        "    genre_matrix = tfidf_g.fit_transform(genre_strings['genre_str'])\n",
        "\n",
        "    # SVD сжатие (жанров не так много, 16 компонент хватит за глаза)\n",
        "    N_GENRE_COMPONENTS = 16\n",
        "    svd_g = TruncatedSVD(n_components=N_GENRE_COMPONENTS, random_state=42)\n",
        "    genre_svd = svd_g.fit_transform(genre_matrix)\n",
        "\n",
        "    # Создаем DataFrame с фичами жанров\n",
        "    genre_feat_cols = [f'genre_svd_{i}' for i in range(N_GENRE_COMPONENTS)]\n",
        "    genre_df = pd.DataFrame(genre_svd, columns=genre_feat_cols)\n",
        "    genre_df['book_id'] = genre_strings['book_id']\n",
        "\n",
        "    print(f\"Genre features shape: {genre_df.shape}\")\n",
        "\n",
        "    # === БЛОК 2: АВТОРЫ И ИЗДАТЕЛЬСТВА (Статистики) ===\n",
        "    # Вместо SVD по авторам (их слишком много и матрица разрежена),\n",
        "    # лучше добавить агрегированные статистики.\n",
        "    print(\"Processing Authors & Publishers Stats...\")\n",
        "\n",
        "    # Используем train_df для расчета популярности авторов (Data Leakage тут нет, это глобальные тренды)\n",
        "    # Нам нужно знать глобальный train_df\n",
        "    train_local = pd.read_csv('train.csv') # Грузим локально для расчетов\n",
        "\n",
        "    # Мержим с книгами, чтобы получить author_id\n",
        "    train_w_meta = train_local.merge(books[['book_id', 'author_id', 'publisher']], on='book_id', how='left')\n",
        "\n",
        "    # Статистики по АВТОРАМ\n",
        "    author_stats = train_w_meta.groupby('author_id').agg({\n",
        "        'rating': ['mean', 'count'],\n",
        "        'has_read': 'mean' # Процент дочитываемости автора\n",
        "    }).reset_index()\n",
        "    author_stats.columns = ['author_id', 'author_mean_rating', 'author_book_count', 'author_read_ratio']\n",
        "\n",
        "    # Статистики по ИЗДАТЕЛЬСТВАМ\n",
        "    pub_stats = train_w_meta.groupby('publisher').agg({\n",
        "        'rating': 'mean',\n",
        "        'has_read': 'mean'\n",
        "    }).reset_index()\n",
        "    pub_stats.columns = ['publisher', 'pub_mean_rating', 'pub_read_ratio']\n",
        "\n",
        "    # === СБОРКА ВСЕГО ВМЕСТЕ ===\n",
        "    # Берем базовые книги\n",
        "    final_content_df = books[['book_id', 'author_id', 'publisher', 'publication_year']].copy()\n",
        "\n",
        "    # Присоединяем жанры (SVD)\n",
        "    final_content_df = final_content_df.merge(genre_df, on='book_id', how='left')\n",
        "\n",
        "    # Заполняем пропуски в SVD нулями (для книг без жанров)\n",
        "    final_content_df[genre_feat_cols] = final_content_df[genre_feat_cols].fillna(0.0)\n",
        "\n",
        "    # Присоединяем статистику авторов\n",
        "    final_content_df = final_content_df.merge(author_stats, on='author_id', how='left')\n",
        "\n",
        "    # Присоединяем статистику издательств\n",
        "    final_content_df = final_content_df.merge(pub_stats, on='publisher', how='left')\n",
        "\n",
        "    # Заполняем пропуски в статистиках средними (для новых авторов)\n",
        "    for col in ['author_mean_rating', 'author_read_ratio', 'pub_mean_rating', 'pub_read_ratio']:\n",
        "        final_content_df[col] = final_content_df[col].fillna(final_content_df[col].mean())\n",
        "\n",
        "    final_content_df['author_book_count'] = final_content_df['author_book_count'].fillna(0)\n",
        "\n",
        "    return reduce_mem_usage(final_content_df)\n",
        "\n",
        "# ЗАПУСК ГЕНЕРАЦИИ\n",
        "content_features_v2 = create_rich_content_features()\n",
        "print(\"Content Features Ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KTy-awzNRlo",
        "outputId": "c50f846c-d4ba-458b-ed3a-c7f400ef2677"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Creating Rich Content Features (Genres, Authors, Descs) ---\n",
            "Processing Genres...\n",
            "Genre features shape: (55784, 17)\n",
            "Processing Authors & Publishers Stats...\n",
            "Memory usage reduced to 2.98 MB\n",
            "Content Features Ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "N-U3A7MDDUb5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sparse\n",
        "import implicit\n",
        "from sklearn.model_selection import KFold\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# Маппинги должны быть уже созданы ранее, но на всякий случай обновим их глобально\n",
        "unique_users = train_df['user_id'].unique()\n",
        "unique_items = books_df['book_id'].unique()\n",
        "user2idx = {u: i for i, u in enumerate(unique_users)}\n",
        "item2idx = {i: k for k, i in enumerate(unique_items)}\n",
        "idx2user = {i: u for u, i in user2idx.items()}\n",
        "idx2item = {k: i for i, k in item2idx.items()}\n",
        "num_users, num_items = len(unique_users), len(unique_items)\n",
        "\n",
        "def generate_hybrid_train_data_v8(df, n_hard=40, n_random=10, n_folds=5):\n",
        "    print(f\"--- [v8] Generating Hybrid Train Data (Global Candidates + OOF Scores) ---\")\n",
        "\n",
        "    # === ЭТАП 1: Обучение Глобальной ALS (для выбора кандидатов) ===\n",
        "    print(\"1. Training Global ALS (Candidate Selection)...\")\n",
        "    rows = df['user_id'].map(user2idx).values\n",
        "    cols = df['book_id'].map(item2idx).values\n",
        "    # Создаем полную матрицу взаимодействий\n",
        "    sparse_global = sparse.csr_matrix((np.ones(len(rows)), (rows, cols)), shape=(num_users, num_items))\n",
        "\n",
        "    global_model = implicit.als.AlternatingLeastSquares(\n",
        "        factors=CFG.ALS_FACTORS,\n",
        "        regularization=CFG.ALS_REGULARIZATION,\n",
        "        iterations=CFG.ALS_ITERATIONS,\n",
        "        random_state=CFG.RANDOM_STATE,\n",
        "        use_gpu=torch.cuda.is_available()\n",
        "    )\n",
        "    global_model.fit(sparse_global, show_progress=True)\n",
        "\n",
        "    # === ЭТАП 2: Сборка структуры датасета (Строки) ===\n",
        "    print(f\"2. Constructing Dataset Skeleton (Pos + {n_hard} Hard + {n_random} Random)...\")\n",
        "\n",
        "    # А) Позитивы (Target: 2 = Read, 1 = Planned)\n",
        "    pos_df = df[['user_id', 'book_id', 'has_read']].copy()\n",
        "    pos_df['target'] = pos_df['has_read'].apply(lambda x: 2.0 if x == 1 else 1.0)\n",
        "    pos_df.drop(columns=['has_read'], inplace=True)\n",
        "\n",
        "    # Б) Hard Negatives (Target: 0) - берем топ рекомендаций глобальной модели\n",
        "    # Это самые \"сложные\" примеры, где модель ошибается\n",
        "    all_users_idx = np.arange(num_users)\n",
        "    ids, _ = global_model.recommend(\n",
        "        all_users_idx,\n",
        "        sparse_global[all_users_idx],\n",
        "        N=n_hard,\n",
        "        filter_already_liked_items=True\n",
        "    )\n",
        "\n",
        "    neg_data = []\n",
        "    # Разворачиваем рекомендации\n",
        "    for i, u_idx in enumerate(all_users_idx):\n",
        "        u_id = idx2user[u_idx]\n",
        "        for b_idx in ids[i]:\n",
        "            # Target 0\n",
        "            neg_data.append([u_id, idx2item[b_idx], 0.0])\n",
        "\n",
        "    # В) Random Negatives (Target: 0) - для разнообразия и робастности\n",
        "    rnd_data = []\n",
        "    if n_random > 0:\n",
        "        for u_idx in all_users_idx:\n",
        "            u_id = idx2user[u_idx]\n",
        "            # Выбираем случайные книги (можно оптимизировать, но цикл надежнее)\n",
        "            rand_items_idx = np.random.randint(0, num_items, n_random)\n",
        "            for r_idx in rand_items_idx:\n",
        "                rnd_data.append([u_id, idx2item[r_idx], 0.0])\n",
        "\n",
        "    # Сборка\n",
        "    neg_df = pd.DataFrame(neg_data, columns=['user_id', 'book_id', 'target'])\n",
        "    rnd_df = pd.DataFrame(rnd_data, columns=['user_id', 'book_id', 'target'])\n",
        "\n",
        "    full_df = pd.concat([pos_df, neg_df, rnd_df], ignore_index=True)\n",
        "\n",
        "    # Удаляем дубликаты (Приоритет: Pos > Hard > Random)\n",
        "    full_df.sort_values(by='target', ascending=False, inplace=True)\n",
        "    full_df.drop_duplicates(subset=['user_id', 'book_id'], keep='first', inplace=True)\n",
        "\n",
        "    # Инициализируем колонки для OOF скоров (пока пустые)\n",
        "    full_df['als_score'] = np.nan\n",
        "    full_df['als_item_norm'] = np.nan\n",
        "\n",
        "    # Помечаем реальные позитивы, чтобы знать, что исключать при OOF обучении\n",
        "    # Создаем set для быстрой проверки: (u_idx, b_idx)\n",
        "    real_interactions = set(zip(\n",
        "        pos_df['user_id'].map(user2idx).values,\n",
        "        pos_df['book_id'].map(item2idx).values\n",
        "    ))\n",
        "\n",
        "    print(f\"Skeleton created. Rows: {len(full_df)}. Starting OOF Scoring...\")\n",
        "\n",
        "    # === ЭТАП 3: OOF Scoring (Честный расчет скоров) ===\n",
        "    # Разбиваем ВЕСЬ full_df на фолды. Для каждого фолда обучаем ALS заново,\n",
        "    # ИСКЛЮЧАЯ из обучения те позитивы, которые попали в валидацию.\n",
        "\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=CFG.RANDOM_STATE)\n",
        "\n",
        "    # Исходные данные для матрицы (все взаимодействия)\n",
        "    train_u_indices = df['user_id'].map(user2idx).values\n",
        "    train_b_indices = df['book_id'].map(item2idx).values\n",
        "\n",
        "    fold_cnt = 0\n",
        "    for _, val_idx in kf.split(full_df):\n",
        "        fold_cnt += 1\n",
        "\n",
        "        # Строки, которые мы сейчас будем скорить\n",
        "        current_rows = full_df.iloc[val_idx]\n",
        "\n",
        "        # Находим среди них те, которые являются РЕАЛЬНЫМИ позитивами\n",
        "        # Их нужно \"спрятать\" от ALS\n",
        "        current_u_idxs = current_rows['user_id'].map(user2idx).fillna(-1).astype(int).values\n",
        "        current_b_idxs = current_rows['book_id'].map(item2idx).fillna(-1).astype(int).values\n",
        "\n",
        "        # Собираем пары (u, i), которые надо исключить из обучения\n",
        "        pairs_to_exclude = set()\n",
        "        for u, b in zip(current_u_idxs, current_b_idxs):\n",
        "            if (u, b) in real_interactions:\n",
        "                pairs_to_exclude.add((u, b))\n",
        "\n",
        "        # Строим обучающую матрицу для этого фолда\n",
        "        # Фильтруем исходный train_df\n",
        "        mask_keep = []\n",
        "        for u, b in zip(train_u_indices, train_b_indices):\n",
        "            if (u, b) in pairs_to_exclude:\n",
        "                mask_keep.append(False)\n",
        "            else:\n",
        "                mask_keep.append(True)\n",
        "\n",
        "        mask_keep = np.array(mask_keep)\n",
        "\n",
        "        # Создаем разреженную матрицу (без валидационных позитивов)\n",
        "        sparse_fold = sparse.csr_matrix(\n",
        "            (np.ones(mask_keep.sum()), (train_u_indices[mask_keep], train_b_indices[mask_keep])),\n",
        "            shape=(num_users, num_items)\n",
        "        )\n",
        "\n",
        "        # Обучаем ALS для фолда\n",
        "        model_fold = implicit.als.AlternatingLeastSquares(\n",
        "            factors=CFG.ALS_FACTORS, regularization=CFG.ALS_REGULARIZATION,\n",
        "            iterations=CFG.ALS_ITERATIONS, random_state=CFG.RANDOM_STATE,\n",
        "            use_gpu=torch.cuda.is_available()\n",
        "        )\n",
        "        model_fold.fit(sparse_fold, show_progress=False)\n",
        "\n",
        "        # Предсказываем (Dot Product)\n",
        "        if hasattr(model_fold.user_factors, \"to_numpy\"):\n",
        "            u_fact = model_fold.user_factors.to_numpy()\n",
        "            i_fact = model_fold.item_factors.to_numpy()\n",
        "        else:\n",
        "            u_fact = model_fold.user_factors\n",
        "            i_fact = model_fold.item_factors\n",
        "\n",
        "        # Берем вектора для текущих юзеров и книг (даже если юзер был удален из трейна, вектор будет, хоть и плохой)\n",
        "        # Обработка -1 не нужна, так как skeleton построен на известных ID\n",
        "        vectors_u = u_fact[current_u_idxs]\n",
        "        vectors_b = i_fact[current_b_idxs]\n",
        "\n",
        "        scores = (vectors_u * vectors_b).sum(axis=1)\n",
        "        norms = np.linalg.norm(vectors_b, axis=1)\n",
        "\n",
        "        # Записываем\n",
        "        full_df.iloc[val_idx, full_df.columns.get_loc('als_score')] = scores\n",
        "        full_df.iloc[val_idx, full_df.columns.get_loc('als_item_norm')] = norms\n",
        "\n",
        "        print(f\"Fold {fold_cnt}/{n_folds} scored.\")\n",
        "\n",
        "        # Чистим память\n",
        "        del model_fold, sparse_fold, vectors_u, vectors_b\n",
        "        gc.collect()\n",
        "\n",
        "    full_df = full_df.sample(frac=1, random_state=CFG.RANDOM_STATE).reset_index(drop=True)\n",
        "    print(f\"Hybrid Dataset v8 Ready.\\nTarget Counts:\\n{full_df['target'].value_counts()}\")\n",
        "\n",
        "    return reduce_mem_usage(full_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HXWVKxMDDcBU"
      },
      "outputs": [],
      "source": [
        "# Предварительный расчет ГЛОБАЛЬНЫХ статистик (один раз на весь ноутбук)\n",
        "# Добавляем \"дочитываемость\" (read_ratio) - сильная фича из v4\n",
        "global_book_stats = train_df.groupby('book_id').agg({\n",
        "    'rating': ['mean', 'count'],\n",
        "    'has_read': ['mean', 'sum']\n",
        "})\n",
        "global_book_stats.columns = ['book_mean', 'book_count', 'book_read_ratio', 'book_read_sum']\n",
        "\n",
        "global_user_stats = train_df.groupby('user_id').agg({\n",
        "    'rating': ['mean', 'count'],\n",
        "    'has_read': ['mean', 'sum']\n",
        "})\n",
        "global_user_stats.columns = ['user_mean', 'user_count', 'user_read_ratio', 'user_read_sum']\n",
        "\n",
        "# Глобальные средние для заполнения пропусков\n",
        "GLOBAL_MEAN_RATING = train_df['rating'].mean()\n",
        "GLOBAL_MEAN_READ = train_df['has_read'].mean()\n",
        "\n",
        "def feature_engineering_v8(df):\n",
        "    print(f\"--- Feature Engineering v8 (Enhanced with Genres & Stats) ---\")\n",
        "    out_df = df.copy()\n",
        "\n",
        "    # 1. Merging Content Features (Metadata)\n",
        "    # Проверяем, есть ли расширенные фичи (с жанрами и статистикой авторов)\n",
        "    if 'content_features_v2' in globals() and content_features_v2 is not None:\n",
        "        print(\"Using Rich Content Features (Genres + Author Stats)...\")\n",
        "        out_df = out_df.merge(content_features_v2, on='book_id', how='left')\n",
        "    else:\n",
        "        print(\"WARNING: 'content_features_v2' not found. Using basic books.csv.\")\n",
        "        out_df = out_df.merge(books_df, on='book_id', how='left')\n",
        "\n",
        "    # Мержим данные пользователей\n",
        "    out_df = out_df.merge(users_df, on='user_id', how='left')\n",
        "\n",
        "    # Обработка категориальных признаков (ID должны быть int)\n",
        "    for c in CFG.CAT_COLS:\n",
        "        if c in out_df.columns:\n",
        "            out_df[c] = out_df[c].fillna(-1).astype(int)\n",
        "\n",
        "    # 2. Text Features (BERT/TF-IDF)\n",
        "    # Используем уже подготовленный text_features_df (из описаний книг)\n",
        "    if 'text_features_df' in globals() and text_features_df is not None:\n",
        "        out_df = out_df.merge(text_features_df, on='book_id', how='left')\n",
        "        feat_cols = [c for c in text_features_df.columns if c != 'book_id']\n",
        "        out_df[feat_cols] = out_df[feat_cols].fillna(0.0)\n",
        "\n",
        "    # 3. Statistics (Global Merge)\n",
        "    # Мержим глобальные статистики по взаимодействиям\n",
        "    out_df = out_df.merge(global_book_stats, on='book_id', how='left')\n",
        "    out_df = out_df.merge(global_user_stats, on='user_id', how='left')\n",
        "\n",
        "    # Заполнение пропусков глобальными средними\n",
        "    out_df['book_mean'] = out_df['book_mean'].fillna(GLOBAL_MEAN_RATING)\n",
        "    out_df['book_count'] = out_df['book_count'].fillna(0)\n",
        "    out_df['book_read_ratio'] = out_df['book_read_ratio'].fillna(GLOBAL_MEAN_READ)\n",
        "    out_df['book_read_sum'] = out_df['book_read_sum'].fillna(0)\n",
        "\n",
        "    out_df['user_mean'] = out_df['user_mean'].fillna(GLOBAL_MEAN_RATING)\n",
        "    out_df['user_count'] = out_df['user_count'].fillna(0)\n",
        "    out_df['user_read_ratio'] = out_df['user_read_ratio'].fillna(GLOBAL_MEAN_READ)\n",
        "    out_df['user_read_sum'] = out_df['user_read_sum'].fillna(0)\n",
        "\n",
        "    # 4. Time Features\n",
        "    if CFG.USE_TIME_FEATURES:\n",
        "        cur_year = 2025\n",
        "        if 'publication_year' in out_df.columns:\n",
        "            out_df['publication_year'] = out_df['publication_year'].fillna(cur_year)\n",
        "            out_df['book_age'] = cur_year - out_df['publication_year']\n",
        "            # Клипаем выбросы (книги старше 200 лет считаем просто \"старыми\")\n",
        "            out_df['book_age'] = out_df['book_age'].clip(0, 200)\n",
        "\n",
        "    # 5. Чистка лишнего\n",
        "    # Удаляем текстовые поля и временные метки, чтобы модель не переобучалась на мусоре\n",
        "    drops = ['title', 'author_name', 'description', 'timestamp', 'has_read',\n",
        "             'genre_str', 'genre_name', 'genre_string'] # Добавил возможные артефакты жанров\n",
        "\n",
        "    out_df.drop([c for c in drops if c in out_df.columns], axis=1, inplace=True)\n",
        "\n",
        "    return reduce_mem_usage(out_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_oKY4IZ9_QVP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520,
          "referenced_widgets": [
            "2b78e11e6fe54e9294702f01abbeb8f9",
            "1b127928dfbd41e4b7ab0e7de1938af5",
            "a86d06f1aabe43f19287e872b41590cf",
            "41f3ba5e56054446b6cc2c8d04612946",
            "7971f74e5d254225b44f3a7dab7cdb1e",
            "67b8e49c6fe0409a8af3d9dbf398756a",
            "bad606be105e4fb1933c8e3da17186d8",
            "f0266b86d56d4f0caacfa7d661fc7086",
            "8e014ba2889a4ef68f78886b37595088",
            "23b0015ac5a145a697027265e9766882",
            "dae3f91e21644c22a0690c05bc78d850"
          ]
        },
        "outputId": "604cf498-19e6-4d72-df3c-11cd7a392685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- [v8] Generating Hybrid Train Data (Global Candidates + OOF Scores) ---\n",
            "1. Training Global ALS (Candidate Selection)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b78e11e6fe54e9294702f01abbeb8f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Constructing Dataset Skeleton (Pos + 40 Hard + 0 Random)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-401261207.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  full_df = pd.concat([pos_df, neg_df, rnd_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skeleton created. Rows: 560621. Starting OOF Scoring...\n",
            "Fold 1/5 scored.\n",
            "Fold 2/5 scored.\n",
            "Fold 3/5 scored.\n",
            "Fold 4/5 scored.\n",
            "Fold 5/5 scored.\n",
            "Hybrid Dataset v8 Ready.\n",
            "Target Counts:\n",
            "target\n",
            "0.0    291560\n",
            "2.0    156603\n",
            "1.0    112458\n",
            "Name: count, dtype: int64\n",
            "Memory usage reduced to 11.76 MB\n",
            "--- Feature Engineering v8 (Enhanced with Genres & Stats) ---\n",
            "Using Rich Content Features (Genres + Author Stats)...\n",
            "Memory usage reduced to 117.63 MB\n",
            "\n",
            "Ready for CatBoost Training.\n",
            "Train Shape: (560638, 104)\n",
            "Columns: ['user_id', 'book_id', 'target', 'als_score', 'als_item_norm', 'author_id', 'publisher', 'publication_year', 'genre_svd_0', 'genre_svd_1', 'genre_svd_2', 'genre_svd_3', 'genre_svd_4', 'genre_svd_5', 'genre_svd_6', 'genre_svd_7', 'genre_svd_8', 'genre_svd_9', 'genre_svd_10', 'genre_svd_11', 'genre_svd_12', 'genre_svd_13', 'genre_svd_14', 'genre_svd_15', 'author_mean_rating', 'author_book_count', 'author_read_ratio', 'pub_mean_rating', 'pub_read_ratio', 'gender', 'age', 'tfidf_svd_0', 'tfidf_svd_1', 'tfidf_svd_2', 'tfidf_svd_3', 'tfidf_svd_4', 'tfidf_svd_5', 'tfidf_svd_6', 'tfidf_svd_7', 'tfidf_svd_8', 'tfidf_svd_9', 'tfidf_svd_10', 'tfidf_svd_11', 'tfidf_svd_12', 'tfidf_svd_13', 'tfidf_svd_14', 'tfidf_svd_15', 'tfidf_svd_16', 'tfidf_svd_17', 'tfidf_svd_18', 'tfidf_svd_19', 'tfidf_svd_20', 'tfidf_svd_21', 'tfidf_svd_22', 'tfidf_svd_23', 'tfidf_svd_24', 'tfidf_svd_25', 'tfidf_svd_26', 'tfidf_svd_27', 'tfidf_svd_28', 'tfidf_svd_29', 'tfidf_svd_30', 'tfidf_svd_31', 'bert_pca_0', 'bert_pca_1', 'bert_pca_2', 'bert_pca_3', 'bert_pca_4', 'bert_pca_5', 'bert_pca_6', 'bert_pca_7', 'bert_pca_8', 'bert_pca_9', 'bert_pca_10', 'bert_pca_11', 'bert_pca_12', 'bert_pca_13', 'bert_pca_14', 'bert_pca_15', 'bert_pca_16', 'bert_pca_17', 'bert_pca_18', 'bert_pca_19', 'bert_pca_20', 'bert_pca_21', 'bert_pca_22', 'bert_pca_23', 'bert_pca_24', 'bert_pca_25', 'bert_pca_26', 'bert_pca_27', 'bert_pca_28', 'bert_pca_29', 'bert_pca_30', 'bert_pca_31', 'book_mean', 'book_count', 'book_read_ratio', 'book_read_sum', 'user_mean', 'user_count', 'user_read_ratio', 'user_read_sum', 'book_age']\n"
          ]
        }
      ],
      "source": [
        "# 1. Генерируем \"честный\" обучающий датасет (включая позитивы и негативы)\n",
        "train_dataset = generate_hybrid_train_data_v8(train_df, n_hard=CFG.TOP_K_CANDIDATES, n_random=CFG.NUM_RANDOM_NEGS, n_folds=CFG.N_FOLDS_ALS)\n",
        "# 2. Навешиваем фичи (текстовые + статистики с LOO)\n",
        "# mode='train' включит защиту от лика таргета в статистиках\n",
        "X_train_strict = feature_engineering_v8(train_dataset)\n",
        "\n",
        "print(\"\\nReady for CatBoost Training.\")\n",
        "print(\"Train Shape:\", X_train_strict.shape)\n",
        "print(\"Columns:\", list(X_train_strict.columns))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 1: SETUP, INSTALLS & DATA PREPARATION ---\n",
        "import gc\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostRanker, Pool\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# 1. Установка библиотек (если нет)\n",
        "!pip install lightgbm xgboost\n",
        "# 1. Генерация данных\n",
        "train_dataset_v8 = generate_hybrid_train_data_v8(train_df, n_hard=CFG.TOP_K_CANDIDATES, n_random=CFG.NUM_RANDOM_NEGS, n_folds=CFG.N_FOLDS_ALS)\n",
        "\n",
        "# 2. Генерация фичей\n",
        "X_train_v8 = feature_engineering_v8(train_dataset_v8)\n",
        "\n",
        "print(\"\\n--- Training CatBoost Ranker (v9) ---\")\n",
        "# Сортировка для групповой обработки\n",
        "X_train_v8.sort_values(by='user_id', inplace=True)\n",
        "# 2. Очистка памяти перед стартом\n",
        "print(\"--- Initial Memory Cleaning ---\")\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 3. Подготовка данных\n",
        "# Используем датасет, который ты создал ранее: X_train_v8\n",
        "# Важно: Сортировка по user_id ОБЯЗАТЕЛЬНА для задач ранжирования (LGBM/XGB требуют, чтобы группы шли подряд)\n",
        "print(\"Sorting data by user_id...\")\n",
        "df_work = X_train_v8.copy()\n",
        "df_work.sort_values(by='user_id', inplace=True)\n",
        "\n",
        "# Определяем колонки\n",
        "drop_cols = ['user_id', 'book_id', 'target']\n",
        "feature_cols = [c for c in df_work.columns if c not in drop_cols]\n",
        "cat_cols = [c for c in CFG.CAT_COLS if c in feature_cols]\n",
        "\n",
        "print(f\"Features ({len(feature_cols)}): {feature_cols}\")\n",
        "\n",
        "# 4. Кодирование категориальных признаков (Label Encoding)\n",
        "# Это нужно для XGBoost и LightGBM, так как они не принимают строки напрямую\n",
        "print(\"Encoding categorical features...\")\n",
        "label_encoders = {}\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    # Заполняем пропуски строкой, чтобы энкодер не падал\n",
        "    df_work[col] = df_work[col].fillna(\"Unknown\") if df_work[col].dtype == 'object' else df_work[col].fillna(-1)\n",
        "    # Приводим к строке для унификации перед кодированием\n",
        "    df_work[col] = df_work[col].astype(str)\n",
        "    df_work[col] = le.fit_transform(df_work[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 5. Разделение на Train / Val по пользователям\n",
        "# Нельзя разрывать группы одного юзера\n",
        "unique_users = df_work['user_id'].unique()\n",
        "train_users, val_users = train_test_split(unique_users, test_size=CFG.VAL_SIZE, random_state=CFG.RANDOM_STATE)\n",
        "\n",
        "train_df = df_work[df_work['user_id'].isin(train_users)]\n",
        "val_df = df_work[df_work['user_id'].isin(val_users)]\n",
        "\n",
        "# 6. Подготовка групп (Query Groups) для LGBM и XGBoost\n",
        "# Нам нужно знать, сколько айтемов у каждого юзера подряд\n",
        "print(\"Calculating group sizes...\")\n",
        "train_group_sizes = train_df.groupby('user_id', sort=False).size().values\n",
        "val_group_sizes = val_df.groupby('user_id', sort=False).size().values\n",
        "\n",
        "# Подготовка X и y\n",
        "X_train = train_df[feature_cols]\n",
        "y_train = train_df['target']\n",
        "X_val = val_df[feature_cols]\n",
        "y_val = val_df['target']\n",
        "\n",
        "# Отдельно сохраняем group_id для CatBoost (ему нужен просто столбец)\n",
        "train_group_id = train_df['user_id']\n",
        "val_group_id = val_df['user_id']\n",
        "\n",
        "# Чистим лишнее\n",
        "del df_work\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Data Prepared. Train shape: {X_train.shape}, Val shape: {X_val.shape}\")\n",
        "print(f\"Train Groups: {len(train_group_sizes)}, Val Groups: {len(val_group_sizes)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "05900e63fedc43448c08f67600220775",
            "d095e82db5eb45419b7fb14fdc0b8977",
            "7b0d7b166da54c08841c1fd3d25e50c8",
            "1c46859be2724d5a938f9991c89bf9cf",
            "cf0ae6f053904721af745200e04015f2",
            "c69cc95283d04cb99978f5a677631245",
            "aa6e4197031f498db267c3f5c4519b13",
            "7a715c13f38848ac82a66c270f6d23e5",
            "2bcac2e397904b6c84adb6f5eda1e7f2",
            "f3a057b78b8d4b0cb56fd2af98fc6844",
            "bde9268790be4c75aab79faa582aacb6"
          ]
        },
        "id": "npFqSiNVdlpp",
        "outputId": "a65a1310-16bc-4418-ff61-b5f628ad1870"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "--- [v8] Generating Hybrid Train Data (Global Candidates + OOF Scores) ---\n",
            "1. Training Global ALS (Candidate Selection)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05900e63fedc43448c08f67600220775"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Constructing Dataset Skeleton (Pos + 40 Hard + 0 Random)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-401261207.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  full_df = pd.concat([pos_df, neg_df, rnd_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skeleton created. Rows: 560621. Starting OOF Scoring...\n",
            "Fold 1/5 scored.\n",
            "Fold 2/5 scored.\n",
            "Fold 3/5 scored.\n",
            "Fold 4/5 scored.\n",
            "Fold 5/5 scored.\n",
            "Hybrid Dataset v8 Ready.\n",
            "Target Counts:\n",
            "target\n",
            "0.0    291560\n",
            "2.0    156603\n",
            "1.0    112458\n",
            "Name: count, dtype: int64\n",
            "Memory usage reduced to 11.76 MB\n",
            "--- Feature Engineering v8 (Enhanced with Genres & Stats) ---\n",
            "Using Rich Content Features (Genres + Author Stats)...\n",
            "Memory usage reduced to 117.63 MB\n",
            "\n",
            "--- Training CatBoost Ranker (v9) ---\n",
            "--- Initial Memory Cleaning ---\n",
            "Sorting data by user_id...\n",
            "Features (101): ['als_score', 'als_item_norm', 'author_id', 'publisher', 'publication_year', 'genre_svd_0', 'genre_svd_1', 'genre_svd_2', 'genre_svd_3', 'genre_svd_4', 'genre_svd_5', 'genre_svd_6', 'genre_svd_7', 'genre_svd_8', 'genre_svd_9', 'genre_svd_10', 'genre_svd_11', 'genre_svd_12', 'genre_svd_13', 'genre_svd_14', 'genre_svd_15', 'author_mean_rating', 'author_book_count', 'author_read_ratio', 'pub_mean_rating', 'pub_read_ratio', 'gender', 'age', 'tfidf_svd_0', 'tfidf_svd_1', 'tfidf_svd_2', 'tfidf_svd_3', 'tfidf_svd_4', 'tfidf_svd_5', 'tfidf_svd_6', 'tfidf_svd_7', 'tfidf_svd_8', 'tfidf_svd_9', 'tfidf_svd_10', 'tfidf_svd_11', 'tfidf_svd_12', 'tfidf_svd_13', 'tfidf_svd_14', 'tfidf_svd_15', 'tfidf_svd_16', 'tfidf_svd_17', 'tfidf_svd_18', 'tfidf_svd_19', 'tfidf_svd_20', 'tfidf_svd_21', 'tfidf_svd_22', 'tfidf_svd_23', 'tfidf_svd_24', 'tfidf_svd_25', 'tfidf_svd_26', 'tfidf_svd_27', 'tfidf_svd_28', 'tfidf_svd_29', 'tfidf_svd_30', 'tfidf_svd_31', 'bert_pca_0', 'bert_pca_1', 'bert_pca_2', 'bert_pca_3', 'bert_pca_4', 'bert_pca_5', 'bert_pca_6', 'bert_pca_7', 'bert_pca_8', 'bert_pca_9', 'bert_pca_10', 'bert_pca_11', 'bert_pca_12', 'bert_pca_13', 'bert_pca_14', 'bert_pca_15', 'bert_pca_16', 'bert_pca_17', 'bert_pca_18', 'bert_pca_19', 'bert_pca_20', 'bert_pca_21', 'bert_pca_22', 'bert_pca_23', 'bert_pca_24', 'bert_pca_25', 'bert_pca_26', 'bert_pca_27', 'bert_pca_28', 'bert_pca_29', 'bert_pca_30', 'bert_pca_31', 'book_mean', 'book_count', 'book_read_ratio', 'book_read_sum', 'user_mean', 'user_count', 'user_read_ratio', 'user_read_sum', 'book_age']\n",
            "Encoding categorical features...\n",
            "Calculating group sizes...\n",
            "Data Prepared. Train shape: (504780, 101), Val shape: (55858, 101)\n",
            "Train Groups: 6560, Val Groups: 729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 2: ENSEMBLE TRAINING LOOP (TUNED VERSION) ---\n",
        "\n",
        "# Оптимизированные параметры\n",
        "MODELS_CONFIG = [\n",
        "    {\n",
        "        'name': 'catboost_yeti',\n",
        "        'type': 'catboost',\n",
        "        'params': {\n",
        "            'loss_function': 'YetiRank',\n",
        "            'iterations': 1500,\n",
        "            'learning_rate':   0.2192448884902637,\n",
        "            'depth': 6,\n",
        "            'l2_leaf_reg':5.7,\n",
        "            'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n",
        "            'random_seed': CFG.RANDOM_STATE,\n",
        "            'eval_metric': 'NDCG:top=20',\n",
        "            'verbose': 100,\n",
        "            'early_stopping_rounds': 100\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'catboost_pairlogit',\n",
        "        'type': 'catboost',\n",
        "        'params': {\n",
        "            'loss_function': 'PairLogitPairwise',\n",
        "            'iterations': 1200,\n",
        "            'learning_rate': 0.08,\n",
        "            'depth': 8,\n",
        "            'l2_leaf_reg': 5,\n",
        "            'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n",
        "            'random_seed': CFG.RANDOM_STATE,\n",
        "            'eval_metric': 'NDCG:top=20',\n",
        "            'metric_period': 5,\n",
        "            'verbose': 50,\n",
        "            'early_stopping_rounds': 100\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'lgbm_lambdarank',\n",
        "        'type': 'lgbm',\n",
        "        'params': {\n",
        "            'objective': 'lambdarank',\n",
        "            'metric': 'ndcg',\n",
        "            'eval_at': [20],\n",
        "            'learning_rate': 0.07,\n",
        "            'n_estimators': 3000,\n",
        "            'max_depth': 7,\n",
        "            'num_leaves': 91,\n",
        "            'colsample_bytree': 0.725,\n",
        "            'subsample': 0.8,\n",
        "            'min_child_samples': 33,\n",
        "            'random_state': CFG.RANDOM_STATE,\n",
        "            'verbosity': -1,\n",
        "            'device': 'cpu' if torch.cuda.is_available() else 'cpu'\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'xgboost_pairwise',\n",
        "        'type': 'xgboost',\n",
        "        'params': {\n",
        "            'objective': 'rank:pairwise',\n",
        "            'eval_metric': 'ndcg@20',\n",
        "            'learning_rate': 0.18,\n",
        "            'n_estimators': 3000,\n",
        "            'max_depth': 6,\n",
        "            'subsample': 0.77,\n",
        "            'colsample_bytree': 0.62,\n",
        "            'gamma': 1.0,\n",
        "            'min_child_weight': 5,\n",
        "            'tree_method': 'hist',\n",
        "            'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "            'random_state': CFG.RANDOM_STATE\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "trained_models = {}\n",
        "\n",
        "for config in MODELS_CONFIG:\n",
        "    model_name = config['name']\n",
        "    model_type = config['type']\n",
        "    params = config['params']\n",
        "\n",
        "    print(f\"\\n{'='*20} Training {model_name} ({model_type}) {'='*20}\")\n",
        "\n",
        "    if model_type == 'catboost':\n",
        "        train_pool = Pool(X_train, y_train, group_id=train_group_id, cat_features=cat_cols)\n",
        "        val_pool = Pool(X_val, y_val, group_id=val_group_id, cat_features=cat_cols)\n",
        "\n",
        "        model = CatBoostRanker(**params)\n",
        "        model.fit(train_pool, eval_set=val_pool)\n",
        "\n",
        "        trained_models[model_name] = model\n",
        "        print(f\"Best Score: {model.get_best_score()}\")\n",
        "        del train_pool, val_pool\n",
        "\n",
        "    elif model_type == 'lgbm':\n",
        "        # !!! ВАЖНО: Явно передаем индексы категорий\n",
        "        cat_indices = [X_train.columns.get_loc(c) for c in cat_cols]\n",
        "        print(f\"Passing {len(cat_indices)} categorical features to LGBM...\")\n",
        "\n",
        "        # Передаем индексы категорий при создании Dataset\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, group=train_group_sizes, categorical_feature=cat_indices)\n",
        "        dval = lgb.Dataset(X_val, label=y_val, group=val_group_sizes, reference=dtrain, categorical_feature=cat_indices)\n",
        "\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            valid_sets=[dval],\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=150), lgb.log_evaluation(200)]\n",
        "            # Отсюда аргумент categorical_feature убрали\n",
        "        )\n",
        "\n",
        "        trained_models[model_name] = model\n",
        "        del dtrain, dval\n",
        "\n",
        "    elif model_type == 'xgboost':\n",
        "        dtrain = xgb.DMatrix(X_train, y_train)\n",
        "        dtrain.set_group(train_group_sizes)\n",
        "\n",
        "        dval = xgb.DMatrix(X_val, y_val)\n",
        "        dval.set_group(val_group_sizes)\n",
        "\n",
        "        model = xgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            num_boost_round=params['n_estimators'],\n",
        "            evals=[(dtrain, 'train'), (dval, 'valid')],\n",
        "            early_stopping_rounds=150, # Увеличили терпение\n",
        "            verbose_eval=200\n",
        "        )\n",
        "\n",
        "        trained_models[model_name] = model\n",
        "        del dtrain, dval\n",
        "\n",
        "    # Очистка памяти\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Finished {model_name}. Memory cleaned.\")\n",
        "\n",
        "print(\"\\nAll models trained successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVrFQ_hldoWx",
        "outputId": "dba0f1bc-75a3-4066-9413-63006ba9b874"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== Training catboost_yeti (catboost) ====================\n",
            "Groupwise loss function. OneHotMaxSize set to 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because NDCG is/are not implemented for GPU\n",
            "Metric NDCG:type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "Metric NDCG:top=20;type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.7544952\tbest: 0.7544952 (0)\ttotal: 117ms\tremaining: 2m 54s\n",
            "100:\ttest: 0.8894124\tbest: 0.8895586 (97)\ttotal: 6.91s\tremaining: 1m 35s\n",
            "200:\ttest: 0.8959444\tbest: 0.8960115 (199)\ttotal: 13.4s\tremaining: 1m 26s\n",
            "300:\ttest: 0.9000355\tbest: 0.9000519 (299)\ttotal: 20.4s\tremaining: 1m 21s\n",
            "400:\ttest: 0.9006124\tbest: 0.9007113 (397)\ttotal: 26.9s\tremaining: 1m 13s\n",
            "500:\ttest: 0.9027938\tbest: 0.9033273 (497)\ttotal: 33s\tremaining: 1m 5s\n",
            "600:\ttest: 0.9038371\tbest: 0.9038630 (599)\ttotal: 39.7s\tremaining: 59.4s\n",
            "700:\ttest: 0.9046712\tbest: 0.9046722 (698)\ttotal: 45.6s\tremaining: 52s\n",
            "800:\ttest: 0.9046543\tbest: 0.9048384 (741)\ttotal: 52.2s\tremaining: 45.5s\n",
            "900:\ttest: 0.9050124\tbest: 0.9050782 (835)\ttotal: 58.1s\tremaining: 38.6s\n",
            "1000:\ttest: 0.9052322\tbest: 0.9053601 (992)\ttotal: 1m 4s\tremaining: 32.2s\n",
            "1100:\ttest: 0.9054583\tbest: 0.9056199 (1062)\ttotal: 1m 10s\tremaining: 25.6s\n",
            "bestTest = 0.9056199216\n",
            "bestIteration = 1062\n",
            "Shrink model to first 1063 iterations.\n",
            "Best Score: {}\n",
            "Finished catboost_yeti. Memory cleaned.\n",
            "\n",
            "==================== Training catboost_pairlogit (catboost) ====================\n",
            "Groupwise loss function. OneHotMaxSize set to 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Metric NDCG:top=20;type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.7935516\tbest: 0.7935516 (0)\ttotal: 1.12s\tremaining: 22m 27s\n",
            "50:\ttest: 0.8905744\tbest: 0.8905744 (50)\ttotal: 54.7s\tremaining: 20m 32s\n",
            "100:\ttest: 0.8957470\tbest: 0.8962133 (97)\ttotal: 1m 48s\tremaining: 19m 35s\n",
            "150:\ttest: 0.8986981\tbest: 0.8986981 (150)\ttotal: 2m 42s\tremaining: 18m 49s\n",
            "200:\ttest: 0.9011359\tbest: 0.9014531 (199)\ttotal: 3m 36s\tremaining: 17m 57s\n",
            "250:\ttest: 0.9022394\tbest: 0.9024029 (234)\ttotal: 4m 31s\tremaining: 17m 5s\n",
            "300:\ttest: 0.9022058\tbest: 0.9026235 (297)\ttotal: 5m 25s\tremaining: 16m 11s\n",
            "350:\ttest: 0.9025578\tbest: 0.9026889 (345)\ttotal: 6m 19s\tremaining: 15m 17s\n",
            "400:\ttest: 0.9031613\tbest: 0.9035811 (369)\ttotal: 7m 12s\tremaining: 14m 22s\n",
            "450:\ttest: 0.9042164\tbest: 0.9042164 (450)\ttotal: 8m 6s\tremaining: 13m 27s\n",
            "500:\ttest: 0.9045044\tbest: 0.9046753 (480)\ttotal: 9m\tremaining: 12m 33s\n",
            "550:\ttest: 0.9054029\tbest: 0.9058966 (538)\ttotal: 9m 53s\tremaining: 11m 39s\n",
            "600:\ttest: 0.9050599\tbest: 0.9058966 (538)\ttotal: 10m 47s\tremaining: 10m 45s\n",
            "bestTest = 0.9058965556\n",
            "bestIteration = 538\n",
            "Shrink model to first 539 iterations.\n",
            "Best Score: {'learn': {'PairLogit': 0.29759624889527125}, 'validation': {'NDCG:top=20;type=Base': 0.9058965555858953, 'PairLogit': 0.3323180711907141}}\n",
            "Finished catboost_pairlogit. Memory cleaned.\n",
            "\n",
            "==================== Training lgbm_lambdarank (lgbm) ====================\n",
            "Passing 2 categorical features to LGBM...\n",
            "Training until validation scores don't improve for 150 rounds\n",
            "[200]\tvalid_0's ndcg@20: 0.888047\n",
            "Early stopping, best iteration is:\n",
            "[120]\tvalid_0's ndcg@20: 0.888369\n",
            "Finished lgbm_lambdarank. Memory cleaned.\n",
            "\n",
            "==================== Training xgboost_pairwise (xgboost) ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [19:22:09] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-ndcg@20:0.82777\tvalid-ndcg@20:0.83285\n",
            "[200]\ttrain-ndcg@20:0.90575\tvalid-ndcg@20:0.88853\n",
            "[337]\ttrain-ndcg@20:0.91554\tvalid-ndcg@20:0.88691\n",
            "Finished xgboost_pairwise. Memory cleaned.\n",
            "\n",
            "All models trained successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 3: INFERENCE, BLENDING & SUBMISSION ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sparse\n",
        "import implicit\n",
        "import gc\n",
        "\n",
        "BLEND_WEIGHTS = {\n",
        "    'catboost_yeti': 0.5,\n",
        "    'catboost_pairlogit': 0.3,\n",
        "    'lgbm_lambdarank': 0.1,\n",
        "    'xgboost_pairwise': 0.1\n",
        "}\n",
        "\n",
        "print(\"--- [Final] Preparing Test Data ---\")\n",
        "\n",
        "# 1. Сборка тестового датафрейма\n",
        "candidates_exploded = []\n",
        "candidates_source = candidates_df.copy() # Из загруженного в начале файла\n",
        "\n",
        "for _, row in candidates_source.iterrows():\n",
        "    u_id = row['user_id']\n",
        "    if pd.isna(row['book_id_list']): continue\n",
        "    b_ids = str(row['book_id_list']).split(',')\n",
        "    for b_id in b_ids:\n",
        "        if b_id: candidates_exploded.append([u_id, int(b_id)])\n",
        "\n",
        "test_df = pd.DataFrame(candidates_exploded, columns=['user_id', 'book_id'])\n",
        "print(f\"Test pairs generated: {len(test_df)}\")\n",
        "\n",
        "# 2. Пересчет ALS score для теста\n",
        "print(\"Recalculating ALS scores for test...\")\n",
        "\n",
        "# Строим матрицу на трейне\n",
        "train_full_local = pd.read_csv('train.csv')\n",
        "u_idx_arr = train_full_local['user_id'].map(user2idx).values\n",
        "i_idx_arr = train_full_local['book_id'].map(item2idx).values\n",
        "sparse_M = sparse.csr_matrix((np.ones(len(u_idx_arr)), (u_idx_arr, i_idx_arr)), shape=(num_users, num_items))\n",
        "\n",
        "# Обучаем быструю ALS\n",
        "model_als_test = implicit.als.AlternatingLeastSquares(\n",
        "    factors=CFG.ALS_FACTORS,\n",
        "    iterations=15, # Чуть больше итераций для точности\n",
        "    random_state=CFG.RANDOM_STATE,\n",
        "    use_gpu=torch.cuda.is_available()\n",
        ")\n",
        "model_als_test.fit(sparse_M, show_progress=False)\n",
        "\n",
        "# Предсказываем (Dot Product)\n",
        "u_test_idx = test_df['user_id'].map(user2idx).fillna(-1).astype(int).values\n",
        "b_test_idx = test_df['book_id'].map(item2idx).fillna(-1).astype(int).values\n",
        "mask = (u_test_idx >= 0) & (b_test_idx >= 0)\n",
        "\n",
        "scores = np.zeros(len(test_df))\n",
        "norms = np.zeros(len(test_df))\n",
        "\n",
        "# --- ИСПРАВЛЕНИЕ ОШИБКИ ЗДЕСЬ (if -> itf) ---\n",
        "if hasattr(model_als_test.user_factors, \"to_numpy\"):\n",
        "    uf = model_als_test.user_factors.to_numpy()\n",
        "    itf = model_als_test.item_factors.to_numpy() # Исправлено имя переменной\n",
        "else:\n",
        "    uf = model_als_test.user_factors\n",
        "    itf = model_als_test.item_factors # Исправлено имя переменной\n",
        "\n",
        "scores[mask] = (uf[u_test_idx[mask]] * itf[b_test_idx[mask]]).sum(axis=1)\n",
        "norms[mask] = np.linalg.norm(itf[b_test_idx[mask]], axis=1)\n",
        "\n",
        "test_df['als_score'] = scores\n",
        "test_df['als_item_norm'] = norms\n",
        "\n",
        "# Чистим память от ALS\n",
        "del model_als_test, sparse_M, uf, itf\n",
        "gc.collect()\n",
        "\n",
        "# 3. Генерация фичей\n",
        "print(\"Generating features for test...\")\n",
        "test_df = feature_engineering_v8(test_df)\n",
        "\n",
        "# 4. Применение LabelEncoder к тесту (для LGBM/XGB)\n",
        "print(\"Applying encodings to test...\")\n",
        "for col in cat_cols:\n",
        "    le = label_encoders.get(col)\n",
        "    if le:\n",
        "        test_df[col] = test_df[col].fillna(\"Unknown\").astype(str)\n",
        "        known_classes = set(le.classes_)\n",
        "        # Заменяем неизвестные категории на самый частый класс (обычно 0) или \"Unknown\" если есть\n",
        "        fallback_val = le.classes_[0]\n",
        "        test_df[col] = test_df[col].apply(lambda x: x if x in known_classes else fallback_val)\n",
        "        test_df[col] = le.transform(test_df[col])\n",
        "\n",
        "# Выравнивание колонок\n",
        "X_test = test_df[feature_cols].copy()\n",
        "\n",
        "# 5. Цикл предсказаний\n",
        "print(\"--- Predicting with Ensemble ---\")\n",
        "preds_df = test_df[['user_id', 'book_id']].copy()\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    print(f\"Predicting: {name}...\")\n",
        "\n",
        "    try:\n",
        "        if 'catboost' in name:\n",
        "            # CatBoost Pool\n",
        "            test_pool = Pool(X_test, group_id=test_df['user_id'], cat_features=cat_cols)\n",
        "            raw_preds = model.predict(test_pool)\n",
        "\n",
        "        elif 'lgbm' in name:\n",
        "            raw_preds = model.predict(X_test)\n",
        "\n",
        "        elif 'xgboost' in name:\n",
        "            dtest = xgb.DMatrix(X_test)\n",
        "            raw_preds = model.predict(dtest)\n",
        "\n",
        "        preds_df[f'score_{name}'] = raw_preds\n",
        "    except Exception as e:\n",
        "        print(f\"Error predicting with {name}: {e}\")\n",
        "\n",
        "# 6. Статистика и Блендинг\n",
        "score_cols = [c for c in preds_df.columns if c.startswith('score_')]\n",
        "print(\"\\n--- Model Correlations ---\")\n",
        "print(preds_df[score_cols].corr())\n",
        "\n",
        "print(\"\\n--- Predictions Statistics (Raw) ---\")\n",
        "print(preds_df[score_cols].describe())\n",
        "\n",
        "# Rank Normalization\n",
        "print(\"Normalizing ranks...\")\n",
        "for col in score_cols:\n",
        "    preds_df[f'rank_{col}'] = preds_df.groupby('user_id')[col].rank(pct=True)\n",
        "\n",
        "# Взвешенный блендинг\n",
        "print(\"Blending...\")\n",
        "preds_df['final_blend_score'] = 0\n",
        "total_weight = 0\n",
        "\n",
        "for name, weight in BLEND_WEIGHTS.items():\n",
        "    col_name = f'score_{name}'\n",
        "    # Проверяем, есть ли предсказания этой модели (вдруг обучение упало)\n",
        "    if col_name in preds_df.columns:\n",
        "        preds_df['final_blend_score'] += preds_df[f'rank_{col_name}'] * weight\n",
        "        total_weight += weight\n",
        "    else:\n",
        "        print(f\"Warning: {name} predictions not found, skipping in blend.\")\n",
        "\n",
        "if total_weight > 0:\n",
        "    preds_df['final_blend_score'] /= total_weight\n",
        "else:\n",
        "    print(\"Error: No models available for blending!\")\n",
        "\n",
        "# 7. Сохранение\n",
        "def save_submission(df, score_col, filename):\n",
        "    print(f\"Generating {filename}...\")\n",
        "    # Сортируем: сначала User, потом Score (descending)\n",
        "    df_sorted = df.sort_values(['user_id', score_col], ascending=[True, False])\n",
        "    # Убираем дубли книг\n",
        "    df_sorted = df_sorted.drop_duplicates(subset=['user_id', 'book_id'])\n",
        "    # Группируем\n",
        "    submission = df_sorted.groupby('user_id')['book_id'].apply(list).reset_index()\n",
        "    # Форматируем топ-20\n",
        "    submission['book_id_list'] = submission['book_id'].apply(lambda x: \",\".join([str(i) for i in x[:20]]))\n",
        "    # Сохраняем (мердж с candidates_source, чтобы сохранить всех юзеров)\n",
        "    final_sub = candidates_source[['user_id']].merge(submission[['user_id', 'book_id_list']], on='user_id', how='left')\n",
        "    final_sub.to_csv(filename, index=False)\n",
        "    print(f\"Saved to {filename}\")\n",
        "\n",
        "# Сохраняем индивидуальные сабмиты\n",
        "for col in score_cols:\n",
        "    model_name = col.replace('score_', '')\n",
        "    save_submission(preds_df, col, f'submission_{model_name}.csv')\n",
        "\n",
        "# Сохраняем Блендинг\n",
        "if 'final_blend_score' in preds_df.columns:\n",
        "    save_submission(preds_df, 'final_blend_score', 'submission_ENSEMBLE_BLENDED.csv')\n",
        "\n",
        "print(\"\\nDONE! All submissions saved.\")\n",
        "print(preds_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkH_9o1ddyoo",
        "outputId": "4f507284-768c-48a7-82d4-03b3b69b66c7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- [Final] Preparing Test Data ---\n",
            "Test pairs generated: 81048\n",
            "Recalculating ALS scores for test...\n",
            "Generating features for test...\n",
            "--- Feature Engineering v8 (Enhanced with Genres & Stats) ---\n",
            "Using Rich Content Features (Genres + Author Stats)...\n",
            "Memory usage reduced to 16.23 MB\n",
            "Applying encodings to test...\n",
            "--- Predicting with Ensemble ---\n",
            "Predicting: catboost_yeti...\n",
            "Predicting: catboost_pairlogit...\n",
            "Predicting: lgbm_lambdarank...\n",
            "Predicting: xgboost_pairwise...\n",
            "\n",
            "--- Model Correlations ---\n",
            "                          score_catboost_yeti  score_catboost_pairlogit  \\\n",
            "score_catboost_yeti                  1.000000                  0.970146   \n",
            "score_catboost_pairlogit             0.970146                  1.000000   \n",
            "score_lgbm_lambdarank                0.968597                  0.969559   \n",
            "score_xgboost_pairwise               0.954572                  0.965846   \n",
            "\n",
            "                          score_lgbm_lambdarank  score_xgboost_pairwise  \n",
            "score_catboost_yeti                    0.968597                0.954572  \n",
            "score_catboost_pairlogit               0.969559                0.965846  \n",
            "score_lgbm_lambdarank                  1.000000                0.982091  \n",
            "score_xgboost_pairwise                 0.982091                1.000000  \n",
            "\n",
            "--- Predictions Statistics (Raw) ---\n",
            "       score_catboost_yeti  score_catboost_pairlogit  score_lgbm_lambdarank  \\\n",
            "count         81049.000000              81049.000000           81049.000000   \n",
            "mean             -0.298603                  0.858646              -0.084475   \n",
            "std               2.319398                  2.870636               2.559293   \n",
            "min              -4.644345                 -4.530218              -5.684029   \n",
            "25%              -2.336494                 -1.570984              -2.182947   \n",
            "50%              -0.835255                  0.196088              -0.640657   \n",
            "75%               1.661123                  2.883313               1.513191   \n",
            "max               5.307840                  9.329725               8.074984   \n",
            "\n",
            "       score_xgboost_pairwise  \n",
            "count            81049.000000  \n",
            "mean                 0.369915  \n",
            "std                  2.777697  \n",
            "min                 -6.445667  \n",
            "25%                 -1.804148  \n",
            "50%                 -0.349076  \n",
            "75%                  2.021168  \n",
            "max                  9.089561  \n",
            "Normalizing ranks...\n",
            "Blending...\n",
            "Generating submission_catboost_yeti.csv...\n",
            "Saved to submission_catboost_yeti.csv\n",
            "Generating submission_catboost_pairlogit.csv...\n",
            "Saved to submission_catboost_pairlogit.csv\n",
            "Generating submission_lgbm_lambdarank.csv...\n",
            "Saved to submission_lgbm_lambdarank.csv\n",
            "Generating submission_xgboost_pairwise.csv...\n",
            "Saved to submission_xgboost_pairwise.csv\n",
            "Generating submission_ENSEMBLE_BLENDED.csv...\n",
            "Saved to submission_ENSEMBLE_BLENDED.csv\n",
            "\n",
            "DONE! All submissions saved.\n",
            "   user_id  book_id  score_catboost_yeti  score_catboost_pairlogit  \\\n",
            "0      210    11936            -2.353555                 -1.157856   \n",
            "1      210   254097            -1.608727                 -0.426507   \n",
            "2      210   709075            -3.103827                 -1.922914   \n",
            "3      210   840500            -3.038232                 -2.556849   \n",
            "4      210   971259             2.419009                  5.232747   \n",
            "\n",
            "   score_lgbm_lambdarank  score_xgboost_pairwise  rank_score_catboost_yeti  \\\n",
            "0              -1.710313               -1.249981                      0.40   \n",
            "1              -0.326192               -1.571924                      0.55   \n",
            "2              -2.916502               -1.808158                      0.15   \n",
            "3              -2.902649               -2.521749                      0.20   \n",
            "4               4.508474                6.567617                      0.85   \n",
            "\n",
            "   rank_score_catboost_pairlogit  rank_score_lgbm_lambdarank  \\\n",
            "0                           0.45                        0.55   \n",
            "1                           0.65                        0.65   \n",
            "2                           0.25                        0.05   \n",
            "3                           0.15                        0.10   \n",
            "4                           0.90                        1.00   \n",
            "\n",
            "   rank_score_xgboost_pairwise  final_blend_score  \n",
            "0                         0.60              0.450  \n",
            "1                         0.40              0.575  \n",
            "2                         0.35              0.190  \n",
            "3                         0.10              0.165  \n",
            "4                         1.00              0.895  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве финального файла отправки использовать submission_ENSEMBLE_BLENDED.csv"
      ],
      "metadata": {
        "id": "VATX3WXjSbBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все про ии и решение я напишу в файле в корне гита"
      ],
      "metadata": {
        "id": "XN5IOqMNSoY6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nitDiJ1zSv1O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2b78e11e6fe54e9294702f01abbeb8f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b127928dfbd41e4b7ab0e7de1938af5",
              "IPY_MODEL_a86d06f1aabe43f19287e872b41590cf",
              "IPY_MODEL_41f3ba5e56054446b6cc2c8d04612946"
            ],
            "layout": "IPY_MODEL_7971f74e5d254225b44f3a7dab7cdb1e"
          }
        },
        "1b127928dfbd41e4b7ab0e7de1938af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67b8e49c6fe0409a8af3d9dbf398756a",
            "placeholder": "​",
            "style": "IPY_MODEL_bad606be105e4fb1933c8e3da17186d8",
            "value": "100%"
          }
        },
        "a86d06f1aabe43f19287e872b41590cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0266b86d56d4f0caacfa7d661fc7086",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e014ba2889a4ef68f78886b37595088",
            "value": 20
          }
        },
        "41f3ba5e56054446b6cc2c8d04612946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23b0015ac5a145a697027265e9766882",
            "placeholder": "​",
            "style": "IPY_MODEL_dae3f91e21644c22a0690c05bc78d850",
            "value": " 20/20 [00:00&lt;00:00, 41.90it/s]"
          }
        },
        "7971f74e5d254225b44f3a7dab7cdb1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67b8e49c6fe0409a8af3d9dbf398756a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bad606be105e4fb1933c8e3da17186d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0266b86d56d4f0caacfa7d661fc7086": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e014ba2889a4ef68f78886b37595088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23b0015ac5a145a697027265e9766882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dae3f91e21644c22a0690c05bc78d850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05900e63fedc43448c08f67600220775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d095e82db5eb45419b7fb14fdc0b8977",
              "IPY_MODEL_7b0d7b166da54c08841c1fd3d25e50c8",
              "IPY_MODEL_1c46859be2724d5a938f9991c89bf9cf"
            ],
            "layout": "IPY_MODEL_cf0ae6f053904721af745200e04015f2"
          }
        },
        "d095e82db5eb45419b7fb14fdc0b8977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c69cc95283d04cb99978f5a677631245",
            "placeholder": "​",
            "style": "IPY_MODEL_aa6e4197031f498db267c3f5c4519b13",
            "value": "100%"
          }
        },
        "7b0d7b166da54c08841c1fd3d25e50c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a715c13f38848ac82a66c270f6d23e5",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bcac2e397904b6c84adb6f5eda1e7f2",
            "value": 20
          }
        },
        "1c46859be2724d5a938f9991c89bf9cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3a057b78b8d4b0cb56fd2af98fc6844",
            "placeholder": "​",
            "style": "IPY_MODEL_bde9268790be4c75aab79faa582aacb6",
            "value": " 20/20 [00:00&lt;00:00, 43.19it/s]"
          }
        },
        "cf0ae6f053904721af745200e04015f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c69cc95283d04cb99978f5a677631245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa6e4197031f498db267c3f5c4519b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a715c13f38848ac82a66c270f6d23e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bcac2e397904b6c84adb6f5eda1e7f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3a057b78b8d4b0cb56fd2af98fc6844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bde9268790be4c75aab79faa582aacb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}